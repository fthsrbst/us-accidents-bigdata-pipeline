{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš— BÃ¼yÃ¼k Veri ve AnalitiÄŸi - DÃ¶nem Projesi\n",
    "## UÃ§tan Uca Data Pipeline: US Accidents Dataset\n",
    "\n",
    "---\n",
    "\n",
    "**Proje BileÅŸenleri:**\n",
    "1. Veri Alma (Kaggle API)\n",
    "2. Veri Temizleme (PySpark)\n",
    "3. kNN SÄ±nÄ±flandÄ±rma + Performans Metrikleri\n",
    "4. K-Means KÃ¼meleme + GÃ¶rselleÅŸtirme\n",
    "5. Ã‡Ä±ktÄ±larÄ±n JSON olarak dÄ±ÅŸa aktarÄ±lmasÄ±\n",
    "6. MongoDB'ye veri yÃ¼kleme (Lokal PC'de)\n",
    "\n",
    "**Teknolojiler:** Google Colab, PySpark, MongoDB (Lokal), Python\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ BÃ¶lÃ¼m 1: Kurulum ve KÃ¼tÃ¼phaneler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli kÃ¼tÃ¼phanelerin kurulumu\n",
    "!pip install pyspark\n",
    "!pip install kaggle\n",
    "!pip install pandas matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KÃ¼tÃ¼phaneleri import etme\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, isnan, isnull, mean, stddev, hour, dayofweek, month, year\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StringType, NumericType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.neighbors import KNeighborsClassifier as SklearnKNN\n",
    "from sklearn.preprocessing import StandardScaler as SklearnScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, silhouette_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Google Colab dosya indirme\n",
    "from google.colab import files\n",
    "\n",
    "print(\"âœ… TÃ¼m kÃ¼tÃ¼phaneler baÅŸarÄ±yla import edildi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”½ BÃ¶lÃ¼m 2: Veri Alma (Data Ingestion)\n",
    "\n",
    "Kaggle API kullanarak US Accidents veri setini indireceÄŸiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle API Kurulumu\n",
    "# 1. Kaggle hesabÄ±nÄ±zdan API token indirin (kaggle.json)\n",
    "# 2. AÅŸaÄŸÄ±daki hÃ¼creyi Ã§alÄ±ÅŸtÄ±rÄ±n ve kaggle.json dosyasÄ±nÄ± yÃ¼kleyin\n",
    "\n",
    "# Kaggle klasÃ¶rÃ¼nÃ¼ oluÅŸtur\n",
    "!mkdir -p ~/.kaggle\n",
    "\n",
    "print(\"ğŸ“ LÃ¼tfen kaggle.json dosyanÄ±zÄ± yÃ¼kleyin:\")\n",
    "print(\"(Kaggle > Settings > API > Create New Token)\")\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle API yapÄ±landÄ±rmasÄ±\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "print(\"âœ… Kaggle API yapÄ±landÄ±rÄ±ldÄ±!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US Accidents veri setini indir\n",
    "print(\"ğŸ“¥ US Accidents veri seti indiriliyor...\")\n",
    "print(\"(Bu iÅŸlem birkaÃ§ dakika sÃ¼rebilir - ~1.2 GB)\")\n",
    "\n",
    "!kaggle datasets download -d sobhanmoosavi/us-accidents\n",
    "\n",
    "# Zip dosyasÄ±nÄ± Ã§Ä±kart\n",
    "!unzip -o us-accidents.zip\n",
    "\n",
    "print(\"\\nâœ… Veri seti baÅŸarÄ±yla indirildi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dosya boyutunu kontrol et\n",
    "import os\n",
    "\n",
    "# CSV dosyasÄ±nÄ± bul\n",
    "csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
    "print(\"ğŸ“ Bulunan CSV dosyalarÄ±:\")\n",
    "for f in csv_files:\n",
    "    size_mb = os.path.getsize(f) / (1024 * 1024)\n",
    "    print(f\"   - {f}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¥ BÃ¶lÃ¼m 3: PySpark Oturumu BaÅŸlatma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark oturumu oluÅŸtur\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"US_Accidents_BigData_Pipeline\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Spark versiyonunu gÃ¶ster\n",
    "print(f\"âœ… Spark Versiyonu: {spark.version}\")\n",
    "print(f\"âœ… Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi PySpark DataFrame olarak yÃ¼kle\n",
    "print(\"ğŸ“Š Veri yÃ¼kleniyor...\")\n",
    "\n",
    "# CSV dosyasÄ±nÄ± bul ve yÃ¼kle\n",
    "csv_file = [f for f in os.listdir('.') if f.endswith('.csv') and 'accident' in f.lower()][0]\n",
    "print(f\"ğŸ“ YÃ¼klenen dosya: {csv_file}\")\n",
    "\n",
    "df_spark = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
    "\n",
    "print(f\"\\nâœ… Veri baÅŸarÄ±yla yÃ¼klendi!\")\n",
    "print(f\"ğŸ“Š Toplam kayÄ±t sayÄ±sÄ±: {df_spark.count():,}\")\n",
    "print(f\"ğŸ“Š Toplam sÃ¼tun sayÄ±sÄ±: {len(df_spark.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri ÅŸemasÄ±nÄ± gÃ¶rÃ¼ntÃ¼le\n",
    "print(\"ğŸ“‹ Veri ÅemasÄ±:\")\n",
    "print(\"=\"*60)\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ä°lk 5 kaydÄ± gÃ¶rÃ¼ntÃ¼le\n",
    "print(\"ğŸ“‹ Ä°lk 5 KayÄ±t:\")\n",
    "df_spark.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temel istatistikler\n",
    "print(\"ğŸ“Š Temel Ä°statistikler:\")\n",
    "df_spark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§¹ BÃ¶lÃ¼m 4: Veri Temizleme ve Ã–n Ä°ÅŸleme (PySpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksik deÄŸer analizi (OPTÄ°MÄ°ZE EDÄ°LMÄ°Å - tek seferde hesaplama)\n",
    "print(\"ğŸ“Š Eksik DeÄŸer Analizi:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Toplam satÄ±r sayÄ±sÄ±\n",
    "total_rows = df_spark.count()\n",
    "print(f\"ğŸ“Š Toplam satÄ±r: {total_rows:,}\")\n",
    "\n",
    "# TÃœM sÃ¼tunlarÄ±n null deÄŸerlerini TEK SEFERDE hesapla (Ã§ok daha hÄ±zlÄ±!)\n",
    "from pyspark.sql.functions import sum as spark_sum, when, col, isnan\n",
    "\n",
    "# Her sÃ¼tun iÃ§in null sayÄ±sÄ±nÄ± hesapla\n",
    "null_counts_expr = []\n",
    "for column in df_spark.columns:\n",
    "    # isNull kontrolÃ¼ (tÃ¼m tipler iÃ§in)\n",
    "    null_counts_expr.append(\n",
    "        spark_sum(when(col(column).isNull(), 1).otherwise(0)).alias(column)\n",
    "    )\n",
    "\n",
    "# Tek seferde tÃ¼m null'larÄ± say\n",
    "null_counts_row = df_spark.select(null_counts_expr).collect()[0]\n",
    "\n",
    "# SonuÃ§larÄ± DataFrame'e Ã§evir\n",
    "null_counts = []\n",
    "for column in df_spark.columns:\n",
    "    null_count = null_counts_row[column]\n",
    "    if null_count > 0:\n",
    "        null_percent = (null_count / total_rows) * 100\n",
    "        null_counts.append((column, null_count, null_percent))\n",
    "\n",
    "# Eksik deÄŸerleri gÃ¶ster\n",
    "null_df = pd.DataFrame(null_counts, columns=['SÃ¼tun', 'Eksik SayÄ±sÄ±', 'Eksik %'])\n",
    "null_df = null_df.sort_values('Eksik %', ascending=False)\n",
    "print(f\"\\nğŸ“‹ Eksik deÄŸer iÃ§eren {len(null_df)} sÃ¼tun bulundu:\")\n",
    "print(null_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analiz iÃ§in Ã¶nemli sÃ¼tunlarÄ± seÃ§\n",
    "# SÄ±nÄ±flandÄ±rma hedefi: Severity (1-4)\n",
    "# KÃ¼meleme Ã¶zellikleri: CoÄŸrafi ve Ã§evresel faktÃ¶rler\n",
    "\n",
    "selected_columns = [\n",
    "    'Severity',           # Hedef deÄŸiÅŸken (1-4)\n",
    "    'Start_Lat',          # Enlem\n",
    "    'Start_Lng',          # Boylam\n",
    "    'Distance(mi)',       # Etkilenen mesafe\n",
    "    'Temperature(F)',     # SÄ±caklÄ±k\n",
    "    'Humidity(%)',        # Nem\n",
    "    'Pressure(in)',       # BasÄ±nÃ§\n",
    "    'Visibility(mi)',     # GÃ¶rÃ¼ÅŸ mesafesi\n",
    "    'Wind_Speed(mph)',    # RÃ¼zgar hÄ±zÄ±\n",
    "    'Precipitation(in)',  # YaÄŸÄ±ÅŸ\n",
    "    'Weather_Condition',  # Hava durumu\n",
    "    'Sunrise_Sunset',     # GÃ¼ndÃ¼z/Gece\n",
    "    'Civil_Twilight',     # AlacakaranlÄ±k\n",
    "    'State'               # Eyalet\n",
    "]\n",
    "\n",
    "# SeÃ§ili sÃ¼tunlarla yeni DataFrame oluÅŸtur\n",
    "df_selected = df_spark.select(selected_columns)\n",
    "print(f\"âœ… {len(selected_columns)} sÃ¼tun seÃ§ildi.\")\n",
    "df_selected.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri temizleme iÅŸlemleri\n",
    "print(\"ğŸ§¹ Veri Temizleme BaÅŸlÄ±yor...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Eksik deÄŸerleri temizle (sadece sayÄ±sal sÃ¼tunlar iÃ§in ortalama ile doldur)\n",
    "numeric_columns = ['Start_Lat', 'Start_Lng', 'Distance(mi)', 'Temperature(F)', \n",
    "                   'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', \n",
    "                   'Wind_Speed(mph)', 'Precipitation(in)']\n",
    "\n",
    "# Ortalama deÄŸerleri hesapla ve doldur\n",
    "df_cleaned = df_selected\n",
    "\n",
    "for col_name in numeric_columns:\n",
    "    mean_val = df_cleaned.select(mean(col(col_name))).collect()[0][0]\n",
    "    if mean_val is not None:\n",
    "        df_cleaned = df_cleaned.fillna({col_name: mean_val})\n",
    "\n",
    "# 2. Kategorik deÄŸiÅŸkenlerdeki eksik deÄŸerleri 'Unknown' ile doldur\n",
    "categorical_columns = ['Weather_Condition', 'Sunrise_Sunset', 'Civil_Twilight', 'State']\n",
    "for col_name in categorical_columns:\n",
    "    df_cleaned = df_cleaned.fillna({col_name: 'Unknown'})\n",
    "\n",
    "# 3. Severity deÄŸeri null olan satÄ±rlarÄ± kaldÄ±r (hedef deÄŸiÅŸken)\n",
    "df_cleaned = df_cleaned.filter(col('Severity').isNotNull())\n",
    "\n",
    "print(f\"âœ… Temizleme sonrasÄ± kayÄ±t sayÄ±sÄ±: {df_cleaned.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kategorik deÄŸiÅŸkenleri sayÄ±sallaÅŸtÄ±r\n",
    "print(\"ğŸ”„ Kategorik deÄŸiÅŸkenler sayÄ±sallaÅŸtÄ±rÄ±lÄ±yor...\")\n",
    "\n",
    "# StringIndexer ile kategorik deÄŸiÅŸkenleri encode et\n",
    "indexers = []\n",
    "for col_name in categorical_columns:\n",
    "    indexer = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_Index\", handleInvalid=\"keep\")\n",
    "    indexers.append(indexer)\n",
    "\n",
    "# Pipeline oluÅŸtur ve uygula\n",
    "indexer_pipeline = Pipeline(stages=indexers)\n",
    "df_indexed = indexer_pipeline.fit(df_cleaned).transform(df_cleaned)\n",
    "\n",
    "print(\"âœ… Kategorik deÄŸiÅŸkenler encode edildi!\")\n",
    "df_indexed.select([f\"{c}_Index\" for c in categorical_columns]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab bellek yÃ¶netimi iÃ§in veriyi Ã¶rnekle\n",
    "# Tam veri seti Ã§ok bÃ¼yÃ¼k olduÄŸundan, analiz iÃ§in Ã¶rnek alÄ±yoruz\n",
    "\n",
    "SAMPLE_SIZE = 500000  # 500K kayÄ±t (belleÄŸe gÃ¶re ayarlayabilirsiniz)\n",
    "\n",
    "print(f\"ğŸ“Š Analiz iÃ§in {SAMPLE_SIZE:,} kayÄ±t Ã¶rnekleniyor...\")\n",
    "df_sample = df_indexed.sample(fraction=SAMPLE_SIZE/df_indexed.count(), seed=42)\n",
    "df_sample = df_sample.limit(SAMPLE_SIZE)\n",
    "\n",
    "print(f\"âœ… Ã–rnek veri boyutu: {df_sample.count():,} kayÄ±t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hedef deÄŸiÅŸken daÄŸÄ±lÄ±mÄ± (Severity)\n",
    "print(\"ğŸ“Š Severity DaÄŸÄ±lÄ±mÄ±:\")\n",
    "severity_dist = df_sample.groupBy('Severity').count().orderBy('Severity')\n",
    "severity_dist.show()\n",
    "\n",
    "# GÃ¶rselleÅŸtirme iÃ§in Pandas'a Ã§evir\n",
    "severity_pd = severity_dist.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(severity_pd['Severity'].astype(str), severity_pd['count'], color=['green', 'yellow', 'orange', 'red'])\n",
    "plt.xlabel('Kaza Åiddeti (Severity)')\n",
    "plt.ylabel('KayÄ±t SayÄ±sÄ±')\n",
    "plt.title('Kaza Åiddeti DaÄŸÄ±lÄ±mÄ±')\n",
    "for i, v in enumerate(severity_pd['count']):\n",
    "    plt.text(i, v + 1000, f'{v:,}', ha='center', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig('severity_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ BÃ¶lÃ¼m 5: kNN SÄ±nÄ±flandÄ±rma + Performans Metrikleri\n",
    "\n",
    "Kaza ÅŸiddetini (Severity 1-4) tahmin etmek iÃ§in kNN algoritmasÄ± kullanacaÄŸÄ±z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã–zellik vektÃ¶rÃ¼ oluÅŸtur\n",
    "feature_columns = [\n",
    "    'Start_Lat', 'Start_Lng', 'Distance(mi)', 'Temperature(F)',\n",
    "    'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)',\n",
    "    'Precipitation(in)', 'Weather_Condition_Index', 'Sunrise_Sunset_Index',\n",
    "    'Civil_Twilight_Index', 'State_Index'\n",
    "]\n",
    "\n",
    "# Eksik deÄŸerleri tekrar kontrol et ve temizle\n",
    "for col_name in feature_columns:\n",
    "    df_sample = df_sample.filter(col(col_name).isNotNull())\n",
    "\n",
    "print(f\"âœ… Temiz veri sayÄ±sÄ±: {df_sample.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark'ta kNN doÄŸrudan desteklenmediÄŸi iÃ§in sklearn kullanacaÄŸÄ±z\n",
    "# Ã–nce veriyi Pandas'a Ã§evirelim\n",
    "\n",
    "print(\"ğŸ“Š Veri sklearn iÃ§in hazÄ±rlanÄ±yor...\")\n",
    "\n",
    "# Daha kÃ¼Ã§Ã¼k bir Ã¶rnek al (bellek iÃ§in)\n",
    "KNN_SAMPLE_SIZE = 100000\n",
    "df_knn_sample = df_sample.limit(KNN_SAMPLE_SIZE)\n",
    "\n",
    "# Pandas'a Ã§evir\n",
    "df_pd = df_knn_sample.select(feature_columns + ['Severity']).toPandas()\n",
    "\n",
    "print(f\"âœ… Pandas DataFrame boyutu: {df_pd.shape}\")\n",
    "df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã–zellik ve hedef deÄŸiÅŸkenleri ayÄ±r\n",
    "X = df_pd[feature_columns].values\n",
    "y = df_pd['Severity'].values\n",
    "\n",
    "# Eksik deÄŸerleri kontrol et ve temizle\n",
    "mask = ~np.isnan(X).any(axis=1)\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "print(f\"âœ… Ã–zellik matrisi boyutu: {X.shape}\")\n",
    "print(f\"âœ… Hedef vektÃ¶r boyutu: {y.shape}\")\n",
    "print(f\"\\nğŸ“Š SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"   Severity {int(u)}: {c:,} ({c/len(y)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test bÃ¶lme\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"âœ… EÄŸitim seti: {X_train.shape[0]:,} kayÄ±t\")\n",
    "print(f\"âœ… Test seti: {X_test.shape[0]:,} kayÄ±t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã–zellik Ã¶lÃ§ekleme (StandardScaler)\n",
    "scaler = SklearnScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"âœ… Ã–zellikler Ã¶lÃ§eklendi (StandardScaler)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN Modeli EÄŸitimi\n",
    "print(\"ğŸ¯ kNN Modeli EÄŸitiliyor...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# FarklÄ± k deÄŸerleri iÃ§in test\n",
    "k_values = [3, 5, 7, 9, 11]\n",
    "results = []\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\nğŸ”„ k={k} iÃ§in model eÄŸitiliyor...\")\n",
    "    \n",
    "    knn = SklearnKNN(n_neighbors=k, weights='distance', n_jobs=-1)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Tahmin\n",
    "    y_pred = knn.predict(X_test_scaled)\n",
    "    \n",
    "    # Accuracy hesapla\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    results.append({'k': k, 'accuracy': accuracy})\n",
    "    \n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# En iyi k deÄŸerini bul\n",
    "best_result = max(results, key=lambda x: x['accuracy'])\n",
    "print(f\"\\nğŸ† En Ä°yi k deÄŸeri: {best_result['k']} (Accuracy: {best_result['accuracy']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En iyi k ile final model\n",
    "BEST_K = best_result['k']\n",
    "\n",
    "print(f\"ğŸ¯ Final kNN Modeli (k={BEST_K})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "knn_final = SklearnKNN(n_neighbors=BEST_K, weights='distance', n_jobs=-1)\n",
    "knn_final.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Tahminler\n",
    "y_pred_final = knn_final.predict(X_test_scaled)\n",
    "y_pred_proba = knn_final.predict_proba(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORMANS METRÄ°KLERÄ°\n",
    "print(\"ğŸ“Š PERFORMANS METRÄ°KLERÄ°\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nğŸ“‹ Classification Report:\")\n",
    "class_report = classification_report(y_test, y_pred_final, target_names=['Severity 1', 'Severity 2', 'Severity 3', 'Severity 4'])\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DetaylÄ± Metrikler\n",
    "accuracy = accuracy_score(y_test, y_pred_final)\n",
    "precision_macro = precision_score(y_test, y_pred_final, average='macro')\n",
    "precision_weighted = precision_score(y_test, y_pred_final, average='weighted')\n",
    "recall_macro = recall_score(y_test, y_pred_final, average='macro')\n",
    "recall_weighted = recall_score(y_test, y_pred_final, average='weighted')\n",
    "f1_macro = f1_score(y_test, y_pred_final, average='macro')\n",
    "f1_weighted = f1_score(y_test, y_pred_final, average='weighted')\n",
    "\n",
    "print(\"ğŸ“Š Ã–ZET METRÄ°KLER:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ… Accuracy:           {accuracy:.4f}\")\n",
    "print(f\"âœ… Precision (Macro):  {precision_macro:.4f}\")\n",
    "print(f\"âœ… Precision (Weighted): {precision_weighted:.4f}\")\n",
    "print(f\"âœ… Recall (Macro):     {recall_macro:.4f}\")\n",
    "print(f\"âœ… Recall (Weighted):  {recall_weighted:.4f}\")\n",
    "print(f\"âœ… F1-Score (Macro):   {f1_macro:.4f}\")\n",
    "print(f\"âœ… F1-Score (Weighted): {f1_weighted:.4f}\")\n",
    "\n",
    "# Metrikleri kaydet\n",
    "knn_metrics = {\n",
    "    'model': 'kNN',\n",
    "    'k_value': int(BEST_K),\n",
    "    'accuracy': float(accuracy),\n",
    "    'precision_macro': float(precision_macro),\n",
    "    'precision_weighted': float(precision_weighted),\n",
    "    'recall_macro': float(recall_macro),\n",
    "    'recall_weighted': float(recall_weighted),\n",
    "    'f1_macro': float(f1_macro),\n",
    "    'f1_weighted': float(f1_weighted),\n",
    "    'test_size': int(len(y_test)),\n",
    "    'train_size': int(len(y_train))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix GÃ¶rselleÅŸtirme\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Severity 1', 'Severity 2', 'Severity 3', 'Severity 4'],\n",
    "            yticklabels=['Severity 1', 'Severity 2', 'Severity 3', 'Severity 4'])\n",
    "plt.xlabel('Tahmin Edilen')\n",
    "plt.ylabel('GerÃ§ek')\n",
    "plt.title(f'kNN Confusion Matrix (k={BEST_K})')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix'i metriklere ekle\n",
    "knn_metrics['confusion_matrix'] = cm.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-AUC Hesaplama (One-vs-Rest)\n",
    "# Ã‡ok sÄ±nÄ±flÄ± ROC iÃ§in etiketleri binarize et\n",
    "classes = np.unique(y_test)\n",
    "y_test_bin = label_binarize(y_test, classes=classes)\n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "# Her sÄ±nÄ±f iÃ§in ROC eÄŸrisi\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "auc_scores = {}\n",
    "\n",
    "for i, (class_val, color) in enumerate(zip(classes, colors)):\n",
    "    if i < y_pred_proba.shape[1]:\n",
    "        fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        auc_scores[f'Severity_{int(class_val)}'] = float(roc_auc)\n",
    "        plt.plot(fpr, tpr, color=color, lw=2,\n",
    "                 label=f'Severity {int(class_val)} (AUC = {roc_auc:.4f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random (AUC = 0.5)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - kNN Multi-class Classification')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Ortalama AUC\n",
    "mean_auc = np.mean(list(auc_scores.values()))\n",
    "print(f\"\\nğŸ“Š Ortalama AUC-ROC: {mean_auc:.4f}\")\n",
    "knn_metrics['auc_roc_mean'] = float(mean_auc)\n",
    "knn_metrics['auc_roc_per_class'] = auc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k deÄŸerlerine gÃ¶re accuracy grafiÄŸi\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_df['k'], results_df['accuracy'], 'bo-', linewidth=2, markersize=10)\n",
    "plt.xlabel('k DeÄŸeri')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('kNN: k DeÄŸerine GÃ¶re Accuracy')\n",
    "plt.xticks(k_values)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# En iyi k'yÄ± iÅŸaretle\n",
    "plt.axvline(x=BEST_K, color='red', linestyle='--', label=f'En Ä°yi k={BEST_K}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('knn_k_accuracy.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¨ BÃ¶lÃ¼m 6: K-Means KÃ¼meleme + GÃ¶rselleÅŸtirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means iÃ§in Ã¶zellik seÃ§imi\n",
    "# CoÄŸrafi ve Ã§evresel faktÃ¶rlere gÃ¶re kazalarÄ± kÃ¼meleyeceÄŸiz\n",
    "\n",
    "kmeans_features = [\n",
    "    'Start_Lat', 'Start_Lng', 'Temperature(F)', 'Humidity(%)',\n",
    "    'Visibility(mi)', 'Wind_Speed(mph)'\n",
    "]\n",
    "\n",
    "# PySpark VectorAssembler ile Ã¶zellik vektÃ¶rÃ¼ oluÅŸtur\n",
    "assembler = VectorAssembler(inputCols=kmeans_features, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "df_kmeans = assembler.transform(df_sample)\n",
    "\n",
    "# Ã–lÃ§ekleme\n",
    "scaler_spark = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "scaler_model = scaler_spark.fit(df_kmeans)\n",
    "df_kmeans_scaled = scaler_model.transform(df_kmeans)\n",
    "\n",
    "print(\"âœ… K-Means iÃ§in Ã¶zellikler hazÄ±rlandÄ±!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method - Optimal k deÄŸerini bul\n",
    "print(\"ğŸ“Š Elbow Method ile Optimal k Belirleme...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Daha kÃ¼Ã§Ã¼k bir Ã¶rnekle Ã§alÄ±ÅŸ (hÄ±z iÃ§in)\n",
    "df_kmeans_sample = df_kmeans_scaled.sample(fraction=0.1, seed=42)\n",
    "\n",
    "k_range = range(2, 11)\n",
    "costs = []\n",
    "\n",
    "for k in k_range:\n",
    "    print(f\"ğŸ”„ k={k} test ediliyor...\")\n",
    "    \n",
    "    kmeans = KMeans(k=k, seed=42, featuresCol=\"scaled_features\", predictionCol=\"cluster\")\n",
    "    model = kmeans.fit(df_kmeans_sample)\n",
    "    \n",
    "    # WSSSE (Within Set Sum of Squared Errors)\n",
    "    cost = model.summary.trainingCost\n",
    "    costs.append(cost)\n",
    "    \n",
    "    print(f\"   WSSSE: {cost:.2f}\")\n",
    "\n",
    "print(\"\\nâœ… Elbow analizi tamamlandÄ±!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow grafiÄŸi\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(list(k_range), costs, 'bo-', linewidth=2, markersize=10)\n",
    "plt.xlabel('KÃ¼me SayÄ±sÄ± (k)')\n",
    "plt.ylabel('WSSSE (Within Set Sum of Squared Errors)')\n",
    "plt.title('K-Means: Elbow Method')\n",
    "plt.xticks(list(k_range))\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Optimal k'yÄ± iÅŸaretle (genellikle dirsek noktasÄ±)\n",
    "optimal_k = 4  # GÃ¶rsel incelemeye gÃ¶re ayarlayÄ±n\n",
    "plt.axvline(x=optimal_k, color='red', linestyle='--', label=f'Optimal k={optimal_k}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('elbow_method.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final K-Means Modeli\n",
    "OPTIMAL_K = 4\n",
    "\n",
    "print(f\"ğŸ¯ Final K-Means Modeli (k={OPTIMAL_K})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "kmeans_final = KMeans(k=OPTIMAL_K, seed=42, featuresCol=\"scaled_features\", predictionCol=\"cluster\")\n",
    "kmeans_model = kmeans_final.fit(df_kmeans_scaled)\n",
    "\n",
    "# KÃ¼me atamalarÄ±\n",
    "df_clustered = kmeans_model.transform(df_kmeans_scaled)\n",
    "\n",
    "# KÃ¼me merkezleri\n",
    "centers = kmeans_model.clusterCenters()\n",
    "print(\"\\nğŸ“ KÃ¼me Merkezleri:\")\n",
    "for i, center in enumerate(centers):\n",
    "    print(f\"   KÃ¼me {i}: {center[:3]}... (ilk 3 Ã¶zellik)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KÃ¼me daÄŸÄ±lÄ±mÄ±\n",
    "print(\"\\nğŸ“Š KÃ¼me DaÄŸÄ±lÄ±mÄ±:\")\n",
    "cluster_dist = df_clustered.groupBy('cluster').count().orderBy('cluster')\n",
    "cluster_dist.show()\n",
    "\n",
    "cluster_pd = cluster_dist.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']\n",
    "plt.bar(cluster_pd['cluster'].astype(str), cluster_pd['count'], color=colors[:len(cluster_pd)])\n",
    "plt.xlabel('KÃ¼me')\n",
    "plt.ylabel('KayÄ±t SayÄ±sÄ±')\n",
    "plt.title('K-Means KÃ¼me DaÄŸÄ±lÄ±mÄ±')\n",
    "for i, v in enumerate(cluster_pd['count']):\n",
    "    plt.text(i, v + 500, f'{v:,}', ha='center', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig('cluster_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KÃ¼melerin coÄŸrafi gÃ¶rselleÅŸtirmesi\n",
    "print(\"ğŸ—ºï¸ CoÄŸrafi KÃ¼me GÃ¶rselleÅŸtirmesi...\")\n",
    "\n",
    "# GÃ¶rselleÅŸtirme iÃ§in Ã¶rnek al\n",
    "df_viz = df_clustered.select('Start_Lat', 'Start_Lng', 'cluster', 'Severity').limit(50000).toPandas()\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "scatter = plt.scatter(df_viz['Start_Lng'], df_viz['Start_Lat'], \n",
    "                      c=df_viz['cluster'], cmap='viridis', alpha=0.5, s=1)\n",
    "plt.colorbar(scatter, label='KÃ¼me')\n",
    "plt.xlabel('Boylam (Longitude)')\n",
    "plt.ylabel('Enlem (Latitude)')\n",
    "plt.title('ABD Trafik KazalarÄ± - K-Means KÃ¼meleme (CoÄŸrafi)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('geographic_clusters.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KÃ¼me karakteristikleri analizi\n",
    "print(\"ğŸ“Š KÃ¼me Karakteristikleri:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cluster_stats = df_clustered.groupBy('cluster').agg(\n",
    "    mean('Temperature(F)').alias('Avg_Temp'),\n",
    "    mean('Humidity(%)').alias('Avg_Humidity'),\n",
    "    mean('Visibility(mi)').alias('Avg_Visibility'),\n",
    "    mean('Wind_Speed(mph)').alias('Avg_Wind'),\n",
    "    mean('Severity').alias('Avg_Severity'),\n",
    "    count('*').alias('Count')\n",
    ").orderBy('cluster')\n",
    "\n",
    "cluster_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KÃ¼me karakteristikleri gÃ¶rselleÅŸtirmesi\n",
    "cluster_stats_pd = cluster_stats.toPandas()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Ortalama SÄ±caklÄ±k\n",
    "axes[0, 0].bar(cluster_stats_pd['cluster'].astype(str), cluster_stats_pd['Avg_Temp'], color='coral')\n",
    "axes[0, 0].set_xlabel('KÃ¼me')\n",
    "axes[0, 0].set_ylabel('Ortalama SÄ±caklÄ±k (Â°F)')\n",
    "axes[0, 0].set_title('KÃ¼melere GÃ¶re Ortalama SÄ±caklÄ±k')\n",
    "\n",
    "# Ortalama Nem\n",
    "axes[0, 1].bar(cluster_stats_pd['cluster'].astype(str), cluster_stats_pd['Avg_Humidity'], color='skyblue')\n",
    "axes[0, 1].set_xlabel('KÃ¼me')\n",
    "axes[0, 1].set_ylabel('Ortalama Nem (%)')\n",
    "axes[0, 1].set_title('KÃ¼melere GÃ¶re Ortalama Nem')\n",
    "\n",
    "# Ortalama GÃ¶rÃ¼ÅŸ Mesafesi\n",
    "axes[1, 0].bar(cluster_stats_pd['cluster'].astype(str), cluster_stats_pd['Avg_Visibility'], color='lightgreen')\n",
    "axes[1, 0].set_xlabel('KÃ¼me')\n",
    "axes[1, 0].set_ylabel('Ortalama GÃ¶rÃ¼ÅŸ (mi)')\n",
    "axes[1, 0].set_title('KÃ¼melere GÃ¶re Ortalama GÃ¶rÃ¼ÅŸ Mesafesi')\n",
    "\n",
    "# Ortalama Kaza Åiddeti\n",
    "axes[1, 1].bar(cluster_stats_pd['cluster'].astype(str), cluster_stats_pd['Avg_Severity'], color='salmon')\n",
    "axes[1, 1].set_xlabel('KÃ¼me')\n",
    "axes[1, 1].set_ylabel('Ortalama Åiddet')\n",
    "axes[1, 1].set_title('KÃ¼melere GÃ¶re Ortalama Kaza Åiddeti')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cluster_characteristics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score hesaplama (sklearn ile)\n",
    "# KÃ¼Ã§Ã¼k bir Ã¶rnekle silhouette hesapla\n",
    "df_sil = df_clustered.select(kmeans_features + ['cluster']).limit(20000).toPandas()\n",
    "df_sil = df_sil.dropna()\n",
    "\n",
    "X_sil = df_sil[kmeans_features].values\n",
    "y_sil = df_sil['cluster'].values\n",
    "\n",
    "sil_score = silhouette_score(X_sil, y_sil)\n",
    "print(f\"ğŸ“Š Silhouette Score: {sil_score:.4f}\")\n",
    "\n",
    "# K-Means metriklerini kaydet\n",
    "kmeans_metrics = {\n",
    "    'model': 'K-Means',\n",
    "    'k_value': int(OPTIMAL_K),\n",
    "    'wssse': float(kmeans_model.summary.trainingCost),\n",
    "    'silhouette_score': float(sil_score),\n",
    "    'cluster_sizes': cluster_pd.to_dict('records'),\n",
    "    'cluster_centers': [center.tolist() for center in centers],\n",
    "    'cluster_stats': cluster_stats_pd.to_dict('records')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ BÃ¶lÃ¼m 7: Ã‡Ä±ktÄ±larÄ± JSON Olarak DÄ±ÅŸa Aktarma\n",
    "\n",
    "TÃ¼m sonuÃ§larÄ± JSON dosyalarÄ± olarak kaydedip bilgisayarÄ±nÄ±za indireceÄŸiz.\n",
    "Bu dosyalarÄ± daha sonra lokal MongoDB'ye yÃ¼kleyeceksiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã‡Ä±ktÄ± klasÃ¶rÃ¼ oluÅŸtur\n",
    "import os\n",
    "OUTPUT_DIR = 'mongodb_export'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“ Ã‡Ä±ktÄ± klasÃ¶rÃ¼ oluÅŸturuldu: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. kNN SonuÃ§larÄ±nÄ± kaydet\n",
    "knn_metrics['timestamp'] = datetime.now().isoformat()\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/knn_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(knn_metrics, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"âœ… kNN sonuÃ§larÄ± kaydedildi: knn_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. K-Means SonuÃ§larÄ±nÄ± kaydet\n",
    "kmeans_metrics['timestamp'] = datetime.now().isoformat()\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/kmeans_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(kmeans_metrics, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"âœ… K-Means sonuÃ§larÄ± kaydedildi: kmeans_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Ham veri Ã¶rneÄŸini kaydet (ilk 10,000 kayÄ±t)\n",
    "print(\"ğŸ“Š Ham veri Ã¶rneÄŸi hazÄ±rlanÄ±yor...\")\n",
    "\n",
    "raw_sample = df_spark.limit(10000).toPandas()\n",
    "raw_sample = raw_sample.where(pd.notnull(raw_sample), None)\n",
    "\n",
    "# Datetime sÃ¼tunlarÄ±nÄ± string'e Ã§evir\n",
    "for col in raw_sample.columns:\n",
    "    if raw_sample[col].dtype == 'datetime64[ns]':\n",
    "        raw_sample[col] = raw_sample[col].astype(str)\n",
    "\n",
    "raw_records = raw_sample.to_dict('records')\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/us_accidents_raw.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(raw_records, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(f\"âœ… Ham veri kaydedildi: us_accidents_raw.json ({len(raw_records):,} kayÄ±t)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. TemizlenmiÅŸ veri Ã¶rneÄŸini kaydet (ilk 10,000 kayÄ±t)\n",
    "print(\"ğŸ“Š TemizlenmiÅŸ veri Ã¶rneÄŸi hazÄ±rlanÄ±yor...\")\n",
    "\n",
    "cleaned_sample = df_sample.limit(10000).toPandas()\n",
    "cleaned_sample = cleaned_sample.where(pd.notnull(cleaned_sample), None)\n",
    "\n",
    "# TÃ¼m sÃ¼tunlarÄ± uygun tipe Ã§evir\n",
    "for col in cleaned_sample.columns:\n",
    "    if cleaned_sample[col].dtype == 'datetime64[ns]':\n",
    "        cleaned_sample[col] = cleaned_sample[col].astype(str)\n",
    "\n",
    "cleaned_records = cleaned_sample.to_dict('records')\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/us_accidents_cleaned.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(cleaned_records, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(f\"âœ… TemizlenmiÅŸ veri kaydedildi: us_accidents_cleaned.json ({len(cleaned_records):,} kayÄ±t)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. KÃ¼melenmiÅŸ veri Ã¶rneÄŸini kaydet\n",
    "print(\"ğŸ“Š KÃ¼melenmiÅŸ veri Ã¶rneÄŸi hazÄ±rlanÄ±yor...\")\n",
    "\n",
    "clustered_sample = df_clustered.select(\n",
    "    'Start_Lat', 'Start_Lng', 'Temperature(F)', 'Humidity(%)',\n",
    "    'Visibility(mi)', 'Wind_Speed(mph)', 'Severity', 'cluster'\n",
    ").limit(10000).toPandas()\n",
    "\n",
    "clustered_sample = clustered_sample.where(pd.notnull(clustered_sample), None)\n",
    "clustered_records = clustered_sample.to_dict('records')\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/us_accidents_clustered.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(clustered_records, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(f\"âœ… KÃ¼melenmiÅŸ veri kaydedildi: us_accidents_clustered.json ({len(clustered_records):,} kayÄ±t)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TÃ¼m dosyalarÄ± listele\n",
    "print(\"\\nğŸ“ OluÅŸturulan Dosyalar:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for f in os.listdir(OUTPUT_DIR):\n",
    "    size_kb = os.path.getsize(f'{OUTPUT_DIR}/{f}') / 1024\n",
    "    print(f\"   ğŸ“„ {f}: {size_kb:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DosyalarÄ± ZIP olarak paketle\n",
    "import shutil\n",
    "\n",
    "# GÃ¶rsel dosyalarÄ±nÄ± da ekle\n",
    "image_files = ['severity_distribution.png', 'confusion_matrix.png', 'roc_curves.png', \n",
    "               'knn_k_accuracy.png', 'elbow_method.png', 'cluster_distribution.png',\n",
    "               'geographic_clusters.png', 'cluster_characteristics.png']\n",
    "\n",
    "for img in image_files:\n",
    "    if os.path.exists(img):\n",
    "        shutil.copy(img, OUTPUT_DIR)\n",
    "\n",
    "# ZIP dosyasÄ± oluÅŸtur\n",
    "shutil.make_archive('bigdata_project_output', 'zip', OUTPUT_DIR)\n",
    "\n",
    "print(\"\\nâœ… TÃ¼m dosyalar ZIP olarak paketlendi: bigdata_project_output.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZIP dosyasÄ±nÄ± indir\n",
    "print(\"ğŸ“¥ ZIP dosyasÄ± indiriliyor...\")\n",
    "files.download('bigdata_project_output.zip')\n",
    "\n",
    "print(\"\\nâœ… Ä°ndirme baÅŸlatÄ±ldÄ±!\")\n",
    "print(\"\\nğŸ“‹ Sonraki AdÄ±mlar:\")\n",
    "print(\"   1. ZIP dosyasÄ±nÄ± bilgisayarÄ±nÄ±za indirin\")\n",
    "print(\"   2. ZIP dosyasÄ±nÄ± Ã§Ä±kartÄ±n\")\n",
    "print(\"   3. Lokal MongoDB'yi baÅŸlatÄ±n\")\n",
    "print(\"   4. 'mongodb_local_import.py' scriptini Ã§alÄ±ÅŸtÄ±rÄ±n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š BÃ¶lÃ¼m 8: Ã–zet ve SonuÃ§lar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROJE Ã–ZETÄ°\n",
    "print(\"=\"*70)\n",
    "print(\"                    ğŸ“Š PROJE Ã–ZETÄ°                    \")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸ“ VERÄ° SETÄ°:\")\n",
    "print(f\"   â€¢ Kaynak: US Accidents Dataset (Kaggle)\")\n",
    "print(f\"   â€¢ Toplam KayÄ±t: ~7.7 milyon\")\n",
    "print(f\"   â€¢ Analiz iÃ§in KullanÄ±lan: {KNN_SAMPLE_SIZE:,} kayÄ±t\")\n",
    "print(f\"   â€¢ Ã–zellik SayÄ±sÄ±: {len(feature_columns)}\")\n",
    "\n",
    "print(\"\\nğŸ¯ kNN SINIFLANDIRMA SONUÃ‡LARI:\")\n",
    "print(f\"   â€¢ En Ä°yi k DeÄŸeri: {BEST_K}\")\n",
    "print(f\"   â€¢ Accuracy: {accuracy:.4f}\")\n",
    "print(f\"   â€¢ Precision (Weighted): {precision_weighted:.4f}\")\n",
    "print(f\"   â€¢ Recall (Weighted): {recall_weighted:.4f}\")\n",
    "print(f\"   â€¢ F1-Score (Weighted): {f1_weighted:.4f}\")\n",
    "print(f\"   â€¢ AUC-ROC (Ortalama): {mean_auc:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ¨ K-MEANS KÃœMELEME SONUÃ‡LARI:\")\n",
    "print(f\"   â€¢ Optimal KÃ¼me SayÄ±sÄ±: {OPTIMAL_K}\")\n",
    "print(f\"   â€¢ WSSSE: {kmeans_model.summary.trainingCost:.2f}\")\n",
    "print(f\"   â€¢ Silhouette Score: {sil_score:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ’¾ DIÅA AKTARILAN DOSYALAR:\")\n",
    "print(f\"   â€¢ knn_results.json\")\n",
    "print(f\"   â€¢ kmeans_results.json\")\n",
    "print(f\"   â€¢ us_accidents_raw.json\")\n",
    "print(f\"   â€¢ us_accidents_cleaned.json\")\n",
    "print(f\"   â€¢ us_accidents_clustered.json\")\n",
    "print(f\"   â€¢ GÃ¶rselleÅŸtirme PNG dosyalarÄ±\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"        âœ… COLAB KISMI TAMAMLANDI - DOSYALARI Ä°NDÄ°RÄ°N        \")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark oturumunu kapat\n",
    "spark.stop()\n",
    "print(\"\\nâœ… Spark oturumu kapatÄ±ldÄ±.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
