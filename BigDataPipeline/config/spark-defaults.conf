# Spark Configuration for Big Data Pipeline

# Master connection
spark.master=spark://spark-master:7077

# Application settings
spark.app.name=BigDataPipeline
spark.driver.memory=2g
spark.executor.memory=2g
spark.executor.cores=2

# MongoDB Spark Connector
spark.mongodb.input.uri=mongodb://admin:admin123@mongodb:27017/bigdata_project.us_accidents?authSource=admin
spark.mongodb.output.uri=mongodb://admin:admin123@mongodb:27017/bigdata_project.us_accidents?authSource=admin

# Kafka settings
spark.kafka.bootstrap.servers=kafka:9092

# HDFS settings
spark.hadoop.fs.defaultFS=hdfs://namenode:8020

# Serialization
spark.serializer=org.apache.spark.serializer.KryoSerializer

# SQL settings
spark.sql.shuffle.partitions=10
spark.sql.adaptive.enabled=true

# Event logging
spark.eventLog.enabled=true
spark.eventLog.dir=hdfs://namenode:8020/spark-logs

# UI settings
spark.ui.port=4040
