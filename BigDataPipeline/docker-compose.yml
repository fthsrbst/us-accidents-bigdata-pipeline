version: "3.8"

services:
  # ============================================
  # HDFS - Hadoop Distributed File System
  # ============================================
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - "9870:9870"
      - "8020:8020"
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
      - ./data:/data
    environment:
      - CLUSTER_NAME=bigdata-cluster
    env_file:
      - ./config/hadoop.env
    networks:
      - bigdata-network

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    ports:
      - "9864:9864"
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
      - ./data:/data
    environment:
      - SERVICE_PRECONDITION=namenode:9870
    env_file:
      - ./config/hadoop.env
    networks:
      - bigdata-network
    depends_on:
      - namenode

  # ============================================
  # HIVE - SQL Query Engine
  # ============================================
  hive-metastore-db:
    image: postgres:14
    container_name: hive-metastore-db
    restart: always
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive123
      POSTGRES_DB: metastore
    volumes:
      - hive_metastore_db:/var/lib/postgresql/data
    networks:
      - bigdata-network

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    restart: always
    ports:
      - "9083:9083"
    environment:
      - SERVICE_PRECONDITION=namenode:9870 hive-metastore-db:5432
    env_file:
      - ./config/hadoop.env
    volumes:
      - ./config/hive-site.xml:/opt/hive/conf/hive-site.xml
    networks:
      - bigdata-network
    depends_on:
      - namenode
      - datanode
      - hive-metastore-db

  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    restart: always
    ports:
      - "10000:10000"
      - "10002:10002"
    environment:
      - SERVICE_PRECONDITION=hive-metastore:9083
      - HIVE_CORE_CONF_javax_jdo_option_ConnectionURL=jdbc:postgresql://hive-metastore-db/metastore
    env_file:
      - ./config/hadoop.env
    volumes:
      - ./config/hive-site.xml:/opt/hive/conf/hive-site.xml
      - ./hive:/opt/hive/scripts
    networks:
      - bigdata-network
    depends_on:
      - hive-metastore

  # ============================================
  # KAFKA - Message Streaming
  # ============================================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    restart: always
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_log:/var/lib/zookeeper/log
    networks:
      - bigdata-network

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka
    restart: always
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - bigdata-network
    depends_on:
      - zookeeper

  # ============================================
  # SPARK - Distributed Processing
  # ============================================
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    restart: always
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
    volumes:
      - ./spark:/opt/spark-apps
      - ./data:/opt/spark-data
      - ./output:/opt/spark-output
      - ./scripts:/opt/spark-scripts
    networks:
      - bigdata-network

  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker
    restart: always
    ports:
      - "8081:8081"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=4g
    volumes:
      - ./spark:/opt/spark-apps
      - ./data:/opt/spark-data
      - ./output:/opt/spark-output
    networks:
      - bigdata-network
    depends_on:
      - spark-master

  # ============================================
  # MONGODB - NoSQL Database
  # ============================================
  mongodb:
    image: mongo:6.0
    container_name: mongodb
    restart: always
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: admin123
      MONGO_INITDB_DATABASE: bigdata_project
    volumes:
      - mongodb_data:/data/db
      - ./scripts/mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js:ro
    networks:
      - bigdata-network

  # ============================================
  # JUPYTER - Development Environment
  # ============================================
  jupyter:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyter
    restart: always
    ports:
      - "8888:8888"
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - SPARK_MASTER=spark://spark-master:7077
    volumes:
      - ./spark:/home/jovyan/work
      - ./data:/home/jovyan/data
      - ./output:/home/jovyan/output
    networks:
      - bigdata-network
    depends_on:
      - spark-master

networks:
  bigdata-network:
    driver: bridge

volumes:
  hadoop_namenode:
  hadoop_datanode:
  hive_metastore_db:
  zookeeper_data:
  zookeeper_log:
  kafka_data:
  mongodb_data:
