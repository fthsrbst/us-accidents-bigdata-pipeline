# ğŸš— BÃ¼yÃ¼k Veri ve AnalitiÄŸi - DÃ¶nem Projesi

## UÃ§tan Uca Data Pipeline: US Accidents Dataset

### Docker-Based Big Data Architecture

---

## ğŸ“‹ Proje Ã–zeti

Bu proje, **US Accidents** veri seti (3GB+) kullanÄ±larak Docker container'larÄ± Ã¼zerinde uÃ§tan uca bir **Data Pipeline** tasarÄ±mÄ± ve uygulamasÄ±nÄ± iÃ§ermektedir.

**KullanÄ±lan Teknolojiler:**

- ğŸ³ **Docker & Docker Compose** - Container orkestrasyon
- ğŸ“¦ **HDFS** - DaÄŸÄ±tÄ±k dosya sistemi
- ğŸ **Apache Hive** - SQL sorgu motoru
- ğŸ“¨ **Apache Kafka** - Streaming mesaj kuyruÄŸu
- âš¡ **Apache Spark** - DaÄŸÄ±tÄ±k veri iÅŸleme
- ğŸƒ **MongoDB** - NoSQL veritabanÄ±

---

## ğŸ—ï¸ Sistem Mimarisi

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA PIPELINE AKIÅI                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  DATA INGESTION                                           â”‚  â”‚
â”‚  â”‚  CSV â†’ Kafka Producer â†’ HDFS                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  DATA STORAGE                                             â”‚  â”‚
â”‚  â”‚  HDFS â†’ Hive External Tables                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  DATA PROCESSING (Spark)                                  â”‚  â”‚
â”‚  â”‚  1. Data Cleaning                                         â”‚  â”‚
â”‚  â”‚  2. kNN Classification                                    â”‚  â”‚
â”‚  â”‚  3. Random Forest Classification                          â”‚  â”‚
â”‚  â”‚  4. K-Means Clustering                                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  DATA STORAGE                                             â”‚  â”‚
â”‚  â”‚  Results â†’ MongoDB Collections                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Veri Seti

| Ã–zellik            | DeÄŸer                                                                               |
| ------------------ | ----------------------------------------------------------------------------------- |
| **Kaynak**         | [Kaggle - US Accidents](https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents) |
| **Boyut**          | ~3 GB                                                                               |
| **KayÄ±t SayÄ±sÄ±**   | ~7.7 milyon kaza kaydÄ±                                                              |
| **Ã–zellik SayÄ±sÄ±** | 46 feature                                                                          |
| **Zaman AralÄ±ÄŸÄ±**  | Åubat 2016 - Mart 2023                                                              |

---

## ğŸ³ Docker Services

| Service            | Port         | AÃ§Ä±klama              |
| ------------------ | ------------ | --------------------- |
| **NameNode**       | 9870, 8020   | HDFS Name Node        |
| **DataNode**       | 9864         | HDFS Data Node        |
| **Hive Metastore** | 9083         | Hive metadata servisi |
| **HiveServer2**    | 10000, 10002 | Hive SQL servisi      |
| **Zookeeper**      | 2181         | Kafka koordinasyonu   |
| **Kafka**          | 9092, 29092  | Mesaj kuyruÄŸu         |
| **Spark Master**   | 8080, 7077   | Spark cluster master  |
| **Spark Worker**   | 8081         | Spark iÅŸÃ§i node       |
| **MongoDB**        | 27017        | NoSQL veritabanÄ±      |
| **Jupyter**        | 8888         | GeliÅŸtirme ortamÄ±     |

---

## ğŸ“ Proje YapÄ±sÄ±

```
BigDataPipeline/
â”œâ”€â”€ docker-compose.yml          # Docker orkestrasyon
â”œâ”€â”€ README.md                   # Bu dosya
â”‚
â”œâ”€â”€ config/                     # KonfigÃ¼rasyon dosyalarÄ±
â”‚   â”œâ”€â”€ hadoop.env              # Hadoop environment
â”‚   â”œâ”€â”€ hive-site.xml           # Hive konfigÃ¼rasyonu
â”‚   â””â”€â”€ spark-defaults.conf     # Spark ayarlarÄ±
â”‚
â”œâ”€â”€ hive/                       # Hive scriptleri
â”‚   â””â”€â”€ create_tables.hql       # Tablo tanÄ±mlarÄ±
â”‚
â”œâ”€â”€ spark/                      # Spark iÅŸleri
â”‚   â”œâ”€â”€ data_cleaning.py        # Veri temizleme
â”‚   â”œâ”€â”€ knn_classification.py   # kNN sÄ±nÄ±flandÄ±rma
â”‚   â”œâ”€â”€ random_forest.py        # Random Forest
â”‚   â””â”€â”€ kmeans_clustering.py    # K-Means kÃ¼meleme
â”‚
â”œâ”€â”€ scripts/                    # YardÄ±mcÄ± scriptler
â”‚   â”œâ”€â”€ kafka_producer.py       # Kafka producer
â”‚   â”œâ”€â”€ hdfs_upload.sh          # HDFS yÃ¼kleme
â”‚   â”œâ”€â”€ mongo-init.js           # MongoDB init
â”‚   â””â”€â”€ run_pipeline.sh         # Pipeline Ã§alÄ±ÅŸtÄ±rma
â”‚
â”œâ”€â”€ data/                       # Veri dizini (CSV buraya)
â””â”€â”€ output/                     # Ã‡Ä±ktÄ± dizini
    â”œâ”€â”€ visualizations/         # GÃ¶rseller
    â””â”€â”€ models/                 # Model dosyalarÄ±
```

---

## ğŸš€ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

### Ã–n Gereksinimler

- Docker Desktop (Windows/Mac) veya Docker Engine (Linux)
- En az 16GB RAM
- En az 20GB boÅŸ disk alanÄ±

### AdÄ±m 1: Veri Setini HazÄ±rlama

```powershell
# CSV dosyasÄ±nÄ± data klasÃ¶rÃ¼ne kopyalayÄ±n
copy "C:\Users\fatih\OneDrive\MasaÃ¼stÃ¼\BuyukVeri\Pipeline_Collab\US_Accidents_March23.csv" ".\data\"
```

### AdÄ±m 2: Docker Servislerini BaÅŸlatma

```powershell
# Proje dizinine gidin
cd BigDataPipeline

# TÃ¼m servisleri baÅŸlatÄ±n
docker-compose up -d

# Servislerin durumunu kontrol edin
docker-compose ps
```

### AdÄ±m 3: HDFS'e Veri YÃ¼kleme

```powershell
# HDFS upload scriptini Ã§alÄ±ÅŸtÄ±rÄ±n
docker exec namenode bash /scripts/hdfs_upload.sh
```

### AdÄ±m 4: Hive TablolarÄ±nÄ± OluÅŸturma

```powershell
# Hive tablolarÄ±nÄ± oluÅŸturun
docker exec hive-server beeline -u jdbc:hive2://localhost:10000 -f /opt/hive/scripts/create_tables.hql
```

### AdÄ±m 5: Spark Pipeline'Ä± Ã‡alÄ±ÅŸtÄ±rma

```powershell
# TÃ¼m pipeline'Ä± Ã§alÄ±ÅŸtÄ±rÄ±n
docker exec spark-master bash /opt/spark-apps/../scripts/run_pipeline.sh

# Veya tek tek:
# 1. Data Cleaning
docker exec spark-master spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.2 /opt/spark-apps/data_cleaning.py

# 2. kNN Classification
docker exec spark-master spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.2 /opt/spark-apps/knn_classification.py

# 3. Random Forest
docker exec spark-master spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.2 /opt/spark-apps/random_forest.py

# 4. K-Means Clustering
docker exec spark-master spark-submit --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.2 /opt/spark-apps/kmeans_clustering.py
```

---

## ğŸ“ˆ Pipeline BileÅŸenleri

### 4.1 Veri Alma (Data Ingestion)

- Kafka Producer ile CSV'den streaming veri akÄ±ÅŸÄ±
- HDFS'e batch veri yÃ¼kleme

### 4.2 Veri Depolama (Data Storage)

- HDFS Ã¼zerinde ham veri depolama
- Hive external table ile SQL eriÅŸimi

### 4.3 Veri Temizleme (Data Cleaning)

- Eksik deÄŸer analizi ve doldurma
- Feature selection (20+ Ã¶zellik)
- Temporal feature extraction (saat, gÃ¼n, ay)
- Kategorik encoding

### 4.4 kNN SÄ±nÄ±flandÄ±rma

- Distributed kNN implementasyonu
- Optimal k seÃ§imi
- **Metrikler:** Accuracy, Precision, Recall, F1-Score

### 4.5 Random Forest SÄ±nÄ±flandÄ±rma

- 100 aÄŸaÃ§lÄ± ensemble model
- Feature importance analizi
- **Metrikler:** Accuracy, Precision, Recall, F1-Score, AUC-ROC

### 4.6 K-Means KÃ¼meleme

- Elbow method ile optimal K
- Silhouette score hesaplama
- CoÄŸrafi kÃ¼me gÃ¶rselleÅŸtirmesi

### 4.7 MongoDB Output

- Ham veri Ã¶rneÄŸi: `us_accidents_raw`
- TemizlenmiÅŸ veri: `us_accidents_cleaned`
- KÃ¼melenmiÅŸ veri: `us_accidents_clustered`
- Model sonuÃ§larÄ±: `knn_results`, `random_forest_results`, `kmeans_results`

---

## ğŸ“Š Beklenen Ã‡Ä±ktÄ±lar

### GÃ¶rselleÅŸtirmeler

- `kmeans_elbow_method.png` - Elbow grafiÄŸi
- `kmeans_geographic_clusters.png` - CoÄŸrafi kÃ¼meleme haritasÄ±
- `kmeans_cluster_distribution.png` - KÃ¼me daÄŸÄ±lÄ±mÄ±
- `rf_feature_importance.png` - Feature importance
- `rf_confusion_matrix.png` - Confusion matrix

### JSON Ã‡Ä±ktÄ±larÄ±

- `knn_results.json` - kNN model metrikleri
- `random_forest_results.json` - RF metrikleri
- `kmeans_results.json` - KÃ¼meleme sonuÃ§larÄ±

---

## ğŸŒ Web ArayÃ¼zleri

| Servis           | URL                   |
| ---------------- | --------------------- |
| Spark Master UI  | http://localhost:8080 |
| HDFS NameNode UI | http://localhost:9870 |
| Jupyter Notebook | http://localhost:8888 |

---

## ğŸ”§ Sorun Giderme

### Docker Bellek HatasÄ±

```powershell
# Docker Desktop ayarlarÄ±ndan bellek limitini artÄ±rÄ±n (en az 12GB)
```

### MongoDB BaÄŸlantÄ± HatasÄ±

```powershell
# MongoDB container'Ä±nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± kontrol edin
docker-compose ps mongodb
docker-compose logs mongodb
```

### Spark Job HatasÄ±

```powershell
# Spark loglarÄ±nÄ± kontrol edin
docker-compose logs spark-master
docker-compose logs spark-worker
```

---

## ğŸ“š Kaynaklar

- [US Accidents Dataset](https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents)
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [Apache Hive Documentation](https://hive.apache.org/)
- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- [MongoDB Documentation](https://docs.mongodb.com/)

---

## ğŸ‘¨â€ğŸ’» Proje Bilgileri

**Ders:** BÃ¼yÃ¼k Veri ve AnalitiÄŸi - DÃ¶nem Projesi

---

_Bu proje eÄŸitim amaÃ§lÄ± hazÄ±rlanmÄ±ÅŸtÄ±r._
